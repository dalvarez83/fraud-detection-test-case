{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "00afde62-4731-4157-a175-f1507d4933ec",
   "metadata": {},
   "source": [
    "# Feature Engineer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "830ff061-a8b6-406a-bd21-a215351460ca",
   "metadata": {},
   "source": [
    "This engineers features on the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a036678-7ed9-4034-8a7e-5b616d66737f",
   "metadata": {},
   "source": [
    "## Installations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ec51101-d716-4b8f-aac9-33c6c349e5ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install feature-engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a93fed6-f705-4176-9aa7-445ffabac750",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install boruta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd59304c-45b0-4fec-a151-c4ac896b6964",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "\n",
    "# sklearn imputation libraries\n",
    "from sklearn.impute import SimpleImputer, KNNImputer\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn import preprocessing\n",
    "\n",
    "# apply Boruta method for dimensionality reduction\n",
    "from boruta import BorutaPy\n",
    "\n",
    "# Use DataPrep function to remove all rows with missing values\n",
    "from dataprep.clean import clean_df\n",
    "\n",
    "# mean data imputer\n",
    "from feature_engine.imputation import MeanMedianImputer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebbf776b-f364-4416-a44d-31274df1a576",
   "metadata": {},
   "source": [
    "### Read in data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7036a53d-ef3e-43b8-9457-d0cba3651a0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read data from csv\n",
    "df = pd.read_csv(\"../data/raw/raw_data.csv\", sep=',', engine='python')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30c161c0-323c-4087-9daa-21d50eed47c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create list of float variables\n",
    "float_vars = list()        \n",
    "for x in df.columns:\n",
    "    if df[x].dtypes == 'float64':\n",
    "        float_vars.append(x)\n",
    "\n",
    "# create list of int variables\n",
    "int_vars = list()        \n",
    "for x in df.columns:\n",
    "    if df[x].dtypes == 'int':\n",
    "        int_vars.append(x)\n",
    "\n",
    "# create list of string variables\n",
    "string_vars = list()        \n",
    "for x in df.columns:\n",
    "    if df[x].dtypes == 'str':\n",
    "        string_vars.append(x)\n",
    "\n",
    "# create list of X variables\n",
    "X_vars = list()\n",
    "for col in df.columns:\n",
    "    if col.startswith('x'):\n",
    "        X_vars.append(col)\n",
    "        \n",
    "print(float_vars)\n",
    "print(int_vars)\n",
    "print(string_vars)\n",
    "print(X_vars)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3677a129-d3f8-45e2-9392-b4e6ea8fe737",
   "metadata": {},
   "source": [
    "### Remove all rows with missing values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb6fc907-0952-4d19-9242-f77bb2f85819",
   "metadata": {},
   "source": [
    "Apply the [Drop Missing Data imputation method](https://feature-engine.trainindata.com/en/latest/user_guide/imputation/DropMissingData.html) from the `feature-engineer` package.\n",
    "\n",
    "Removing rows with nan values from a dataset is a common practice in data science and machine learning projects.\n",
    "\n",
    "You are probably familiar with the use of pandas dropna. You basically take a pandas dataframe or a pandas series, apply dropna, and eliminate those rows that contain nan values in one or more columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d857eb7-03ec-4b64-937b-9cf108800a8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from feature_engine.imputation import DropMissingData\n",
    "\n",
    "dmd = DropMissingData()\n",
    "dmd.fit(df)\n",
    "df_dropna = dmd.transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fd15b36-8687-4bb3-99fc-9756b3efe401",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dropna.to_csv('../data/processed/data_nomissingvalues.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "795a45c2-98f4-4096-9a94-0c699b08d61a",
   "metadata": {},
   "source": [
    "### Replace missing data with median values of the variable"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40a89e52-3e63-45e9-9136-fdd5ae479512",
   "metadata": {},
   "source": [
    "The MeanMedianImputer() replaces missing data by the mean or median value of the variable. It works only with numerical variables. See the method documentation [here](https://feature-engine.trainindata.com/en/latest/api_doc/imputation/MeanMedianImputer.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13d83b19-7c1d-4f6e-91b7-87a846b668e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read data from csv\n",
    "df = pd.read_csv(\"../data/raw/raw_data.csv\", sep=',', engine='python')\n",
    "\n",
    "# impute missing values and standardize values \n",
    "imputer = SimpleImputer(missing_values=np.nan, strategy='median')\n",
    "\n",
    "imputer.fit(df)\n",
    "dfimp = imputer.transform(df)\n",
    "dfimp_df = pd.DataFrame(dfimp, columns=df.columns[:])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fbd45d2-21e6-4c05-b419-5f087c64247b",
   "metadata": {},
   "source": [
    "#### Save resulting dataframes to csv for further analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8793ea1-01db-4e83-8b07-b9ab38a292cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfimp_df.to_csv('../data/processed/data_medianvalsformissing.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59572590-d917-4f8a-8f76-80a4d24b741f",
   "metadata": {},
   "source": [
    "### Replace missing data with mean values of the variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09dbb6a5-c90a-490a-a850-70153b401fe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read data from csv\n",
    "df = pd.read_csv(\"../data/raw/raw_data.csv\", sep=',', engine='python')\n",
    "\n",
    "# impute missing values and standardize values \n",
    "imputer = SimpleImputer(missing_values=np.nan, strategy='mean')\n",
    "\n",
    "imputer.fit(df)\n",
    "dfimp = imputer.transform(df)\n",
    "dfimp_df = pd.DataFrame(dfimp, columns=df.columns[:])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f14f601-3e75-4546-abfb-e85da2e15a8c",
   "metadata": {},
   "source": [
    "#### Save resulting dataframes to csv for further analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22f3c06b-d19d-48dc-a66f-ed755bc61667",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfimp_df.to_csv('../data/processed/data_meanvalsformissing.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bdec9d8-eec5-48c2-926e-93b030e36e5c",
   "metadata": {},
   "source": [
    "### Replace missing data with a random sample extracted from the variable"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2f28f66-557c-4c6a-ba33-93387dbafb87",
   "metadata": {},
   "source": [
    "Use the [Random Sample imputer method](https://feature-engine.trainindata.com/en/latest/user_guide/imputation/RandomSampleImputer.html) from the `feature-engineer` package to impute the missing values with a random sample extracted from the variable.\n",
    "\n",
    "If `seed = 'observation'`, then the random_state should be a variable name or a list of variable names. The seed will be calculated observation per observation, either by adding or multiplying the values of the variables indicated in the `random_state`. Then, a value will be extracted from the train set using that seed and used to replace the NAN in that particular observation. This is the equivalent of `pandas.sample(1, random_state=var1+var2)` if the `seeding_method` is set to add or `pandas.sample(1, random_state=var1*var2)` if the `seeding_method` is set to multiply."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dd9c5fc-e361-4a62-bbf8-4407009481c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from feature_engine.imputation import RandomSampleImputer\n",
    "\n",
    "# Read data from csv\n",
    "df = pd.read_csv(\"../data/raw/raw_data.csv\", sep=',', engine='python')\n",
    "\n",
    "# set up the imputer\n",
    "imputer = RandomSampleImputer(\n",
    "        random_state=['y'],\n",
    "        seed='observation',\n",
    "        seeding_method='add'\n",
    "    )\n",
    "\n",
    "# fit the imputer\n",
    "imputer.fit(df)\n",
    "\n",
    "# transform the data\n",
    "dfimp_random = imputer.transform(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dedf3427-fd76-456e-ab9b-62db2f4e0c8b",
   "metadata": {},
   "source": [
    "#### Save resulting dataframes to csv for further analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b04de0c2-8139-4dc6-b910-cc29073c67a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfimp_random.to_csv('../data/processed/data_randomsampleformissing.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67d9641d-e50f-49f6-8441-cf6b9e29c8e6",
   "metadata": {},
   "source": [
    "### Replace missing data with arbitrary values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be8ebe52-172e-4bb6-b6fe-95984b3db91e",
   "metadata": {},
   "source": [
    "The [Arbitrary Number Imputer method](https://feature-engine.trainindata.com/en/latest/user_guide/imputation/ArbitraryNumberImputer.html) replaces missing data with an arbitrary numerical value determined by the user. It works only with numerical variables.\n",
    "\n",
    "The ArbitraryNumberImputer() can find and impute all numerical variables automatically. Alternatively, you can pass a list of the variables you want to impute to the variables parameter.\n",
    "\n",
    "You can impute all variables with the same number, in which case you need to define the variables to impute in the variables parameter and the imputation number in arbitrary_number parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0da010dc-7dcb-4692-96e0-22419fab721d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from feature_engine.imputation import ArbitraryNumberImputer\n",
    "\n",
    "# Read data from csv\n",
    "df = pd.read_csv(\"../data/raw/raw_data.csv\", sep=',', engine='python')\n",
    "\n",
    "# set up the imputer\n",
    "arbitrary_imputer = ArbitraryNumberImputer(\n",
    "    arbitrary_number=np.min(df), #for exposition, make arbitrary number the minimum value in the dataframe\n",
    "    )\n",
    "\n",
    "# fit the imputer\n",
    "arbitrary_imputer.fit(df)\n",
    "\n",
    "# transform the data\n",
    "dfImp_arbitrary= arbitrary_imputer.transform(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8c703e8-7965-4629-bb84-0a95fbd38067",
   "metadata": {},
   "source": [
    "#### Save resulting dataframes to csv for further analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d06c5da4-b039-44b1-9f29-3973080a9666",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfImp_arbitrary.to_csv('../data/processed/data_arbitraryvalueformissing.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5342d23c-994a-45a0-b2a9-fc663f703d8e",
   "metadata": {},
   "source": [
    "### Add Missing Indicator for columns with missing values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6122647-59c3-4d1c-b241-f9a5e47a4c83",
   "metadata": {},
   "source": [
    "Apply the `Add Missing Indicator` method to add a binary variable indicating if observations are missing (missing indicator). It adds missing indicators to both categorical and numerical variables. See the documentation of the [Add Missing Indicator](https://feature-engine.trainindata.com/en/latest/user_guide/imputation/AddMissingIndicator.html) method.\n",
    "\n",
    "You can select the variables for which the missing indicators should be created passing a variable list to the variables parameter. Alternatively, the imputer will automatically select all variables.\n",
    "\n",
    "The imputer has the option to add missing indicators to all variables or only to those that have missing data in the train set. You can change the behaviour using the parameter missing_only.\n",
    "\n",
    "If `missing_only=True`, missing indicators will be added only to those variables with missing data in the train set. This means that if you passed a variable list to variables and some of those variables did not have missing data, no missing indicators will be added to them. If it is paramount that all variables in your list get their missing indicators, make sure to set missing_only=False.\n",
    "\n",
    "It is recommended to use missing_only=True when not passing a list of variables to impute.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "539f0009-b402-44ce-8e84-94e92a59f6b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from feature_engine.imputation import AddMissingIndicator\n",
    "\n",
    "# set up the imputer\n",
    "addBinary_imputer = AddMissingIndicator(missing_only=True)\n",
    "\n",
    "# fit the imputer\n",
    "addBinary_imputer.fit(df)\n",
    "\n",
    "df_MissingIndicator = addBinary_imputer.transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9958ea81-9889-4ae2-8073-2fc6aba268a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_MissingIndicator.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "391dedb6-be42-4184-8f3e-95da324cd5fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# analyze the results\n",
    "print(df_MissingIndicator.shape)\n",
    "df_MissingIndicator.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eef5c52f-8596-4fb0-8a88-cf12f8c629b4",
   "metadata": {},
   "source": [
    "#### Save resulting dataframes to csv for further analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b0e497d-ac89-4c6d-82f6-acac362dcdae",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_MissingIndicator.to_csv('../data/processed/data_missingindicator.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1fddd28-ace7-4099-a8ad-a0ca87878973",
   "metadata": {},
   "source": [
    "### Apply Boruta method to reduce dimensionality of the dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31196930-249e-4337-8a6e-2e4ec1bec445",
   "metadata": {},
   "source": [
    "Apply the Boruta method for feature selection. Boruta is an all relevant feature selection method, while most other are minimal optimal; this means it tries to find all features carrying information usable for prediction, rather than finding a possibly compact subset of features on which some classifier has a minimal error.\n",
    "\n",
    "Why bother with all relevant feature selection? When you try to understand the phenomenon that made your data, you should care about all factors that contribute to it, not just the bluntest signs of it in context of your methodology (yes, minimal optimal set of features by definition depends on your classifier choice).\n",
    "\n",
    "See documentation of the Boruta method [here](https://github.com/scikit-learn-contrib/boruta_py)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53e7866a-bfa8-4bfc-ac6d-622a43e78d91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# select arbitrary number of selected features following Boruta ranking\n",
    "no_selectedfeatures = int(input(\"Please enter number of features to select \\n\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06279092-7371-4cf5-b1cb-e609f18103b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# impute missing values and standardize values \n",
    "imputer = SimpleImputer(missing_values=np.nan, strategy='median')\n",
    "scaler = StandardScaler()\n",
    "\n",
    "imputer.fit(df[X_vars])\n",
    "Ximp = imputer.transform(df[X_vars])\n",
    "scaler.fit(Ximp)\n",
    "Xscaled = scaler.transform(Ximp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34f08fe5-6f53-4d0e-aa3e-589ec520c487",
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate random forest\n",
    "forest = RandomForestRegressor(n_jobs = -1, max_depth = 5)\n",
    "\n",
    "# fit boruta\n",
    "boruta_selector = BorutaPy(forest, n_estimators = 'auto', random_state = 0)\n",
    "boruta_selector.fit(np.array(Xscaled), np.array(df['y']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ae4fde6-ab1a-43fc-85ea-6d0f94891997",
   "metadata": {},
   "outputs": [],
   "source": [
    "boruta_ranking = boruta_selector.ranking_\n",
    "for i, val in enumerate(boruta_ranking):\n",
    "    if val <= no_selectedfeatures:\n",
    "        print (X_vars[i], val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "533a03c1-a028-4701-8a2f-b2a859abf48d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# select features with a ranking of 5 or higher following application of Boruta method\n",
    "boruta_ranking = boruta_selector.ranking_\n",
    "selected_features = np.array(X_vars)[boruta_ranking <= 5]\n",
    "print(selected_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70b631f5-f98e-4549-8a8c-dc09a3b1a906",
   "metadata": {},
   "outputs": [],
   "source": [
    "### save dataframe following Boruta method for downstream analysis\n",
    "df_Boruta = pd.concat([pd.DataFrame(df, columns=selected_features),\n",
    "                       pd.DataFrame(df, columns=['y'])], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a7ee80d-4c09-4216-afac-bc33917fd8ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_Boruta.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4442ac0f-35db-48ea-9afb-bb8a2a163352",
   "metadata": {},
   "source": [
    "#### Save resulting dataframes to csv for further analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a56893c3-4bdd-4798-bee4-780a57b622ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_Boruta.to_csv('../data/processed/data_boruta.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88edbb29-1bce-4cfa-a66c-9e6600a55bac",
   "metadata": {},
   "source": [
    "### Apply Principal Components Analysis (PCA) to reduce dimensionality"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0d6c6dd-f357-482e-a71d-80cc5c2757a7",
   "metadata": {},
   "source": [
    "Apply PCA to reduce the dimensions of the dataset. Linear dimensionality reduction using Singular Value Decomposition of the data to project it to a lower dimensional space. The input data is centered but not scaled for each feature before applying the SVD.\n",
    "\n",
    "It uses the LAPACK implementation of the full SVD or a randomized truncated SVD by the method of Halko et al. 2009, depending on the shape of the input data and the number of components to extract.\n",
    "\n",
    "Set the `n_components` to 'mle' will interpret svd_solver == 'auto' as svd_solver == 'full'. \n",
    "\n",
    "Set `whiten` option to True (False by default) so that the `omponents_` vectors are multiplied by the square root of `n_samples` and then divided by the singular values to ensure uncorrelated outputs with unit component-wise variances.\n",
    "\n",
    "Whitening will remove some information from the transformed signal (the relative variance scales of the components) but can sometime improve the predictive accuracy of the downstream estimators by making their data respect some hard-wired assumptions.\n",
    "\n",
    "See the sci-kit learn documentation for implementing PCA [here](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c98ef08c-f38e-46d3-aaa2-a932eb846aa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# impute missing values and standardize values \n",
    "imputer = SimpleImputer(missing_values=np.nan, strategy='median')\n",
    "scaler = StandardScaler()\n",
    "\n",
    "imputer.fit(df[X_vars])\n",
    "Ximp = imputer.transform(df[X_vars])\n",
    "scaler.fit(Ximp)\n",
    "Xscaled = scaler.transform(Ximp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "606d4725-6fbc-4a03-9a39-31cdfda380e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# set whiten option to True\n",
    "\n",
    "pca = PCA(n_components=\"mle\", copy=True, whiten=True, \n",
    "          svd_solver='auto', tol=0.0, iterated_power='auto', \n",
    "          n_oversamples=5, power_iteration_normalizer='auto',\n",
    "          random_state=None)\n",
    "\n",
    "df_pca = pca.fit_transform(Xscaled)\n",
    "\n",
    "df_pca.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08632f2d-c637-4fd2-8956-99bbafc47c78",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pca = pd.DataFrame(df_pca)\n",
    "\n",
    "# create column names\n",
    "featurenames = list()\n",
    "for i in range(df_pca.shape[1]):\n",
    "    featurenames.append(f\"x{i}\")\n",
    "df_pca.columns = featurenames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16130670-0de0-4019-8f51-38359600c542",
   "metadata": {},
   "outputs": [],
   "source": [
    "### save dataframe following application of PCA for downstream analysis\n",
    "df_pca2 = pd.concat([pd.DataFrame(df_pca),\n",
    "                       pd.DataFrame(df, columns=['y'])], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba3ed911-f1e2-445c-88d8-c4a4270c0be8",
   "metadata": {},
   "source": [
    "#### Save resulting dataframes to csv for further analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa19cd44-d8da-453b-b414-9199ae401517",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pca2.to_csv('../data/processed/data_pca.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8db69413-3a47-44c2-860a-82cc40a6cc25",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

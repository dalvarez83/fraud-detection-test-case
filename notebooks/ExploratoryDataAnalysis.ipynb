{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ef4f065e-70c0-4e17-945d-d1b073e036fb",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97d761b3-9c2c-4e37-8095-c16a3e594b35",
   "metadata": {},
   "source": [
    "This notebook runs through a series of exploratory data analyses to assess the integrity and quality of the raw data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f3ed0dd-10f9-4cd6-ae04-135326720627",
   "metadata": {},
   "source": [
    "## Installations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6be43d18-c7a7-482d-8f91-ff1b276ca2a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install ydata-profiling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edddea7c-6eb3-45ea-b992-74ee002ec897",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install dataprep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72fa1c31-87a9-4382-b047-a26d028f6f67",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "\n",
    "import json\n",
    "from dataprofiler import Data, Profiler\n",
    "from functools import reduce\n",
    "\n",
    "# Set display of images in the notebook\n",
    "%matplotlib notebook\n",
    "\n",
    "# imputation libraries\n",
    "from sklearn.impute import SimpleImputer, KNNImputer\n",
    "\n",
    "# standardize variables\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler \n",
    "\n",
    "# ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3ff5087-20a5-43c1-9f8d-b2ce17f5af36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check versions\n",
    "print(f'pandas version: {pd.__version__}')\n",
    "print(f'numpy version: {np.__version__}')\n",
    "print(f'seaborn version: {sns.__version__}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4192d0a-335d-40a3-b641-6218f1b7949b",
   "metadata": {},
   "source": [
    "## Basic exploratory data approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4060a9bc-181a-4a65-99bb-b36b230f9c62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read data from csv and set data types (dtype), except the first column -'calldate'- which will be parsed later.\n",
    "df = pd.read_csv(\"../data/raw_data.csv\", sep=',', engine='python')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2963f89-7fef-4974-ac4a-f0ce624a57de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect the dataframe\n",
    "print(df.shape)\n",
    "print(df.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "936fbb9a-3a80-4ffe-8693-0bb68bef9962",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If necessary, convert columns to str without spaces in lower case\n",
    "df.columns = df.columns.str.replace('\\s+', '_').str.lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e91bd515-4633-426b-8cb8-cf916cb792c5",
   "metadata": {},
   "source": [
    "#### Apply manual functions for data quality checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6056c71f-2fda-4751-9f12-1fceb4de2238",
   "metadata": {},
   "outputs": [],
   "source": [
    "### analyze null values\n",
    "def nullvalues(d):\n",
    "    ''' Number of null values for each feature and \n",
    "        percentage of null values for each feature'''\n",
    "    print(\"{:10s}|{:10s}|{:10s}\".format(\"Feature\",\n",
    "                                        \"Null values\",\n",
    "                                        \"Null Values as a Proportion of Total\"))\n",
    "    print(\"=\"*100)\n",
    "    for col in d.columns:\n",
    "        null_values = d[col].isnull().sum(axis=0)\n",
    "        null_values_pct = d[col].isnull().sum(axis=0)/len(d)*100\n",
    "        print(\"{:10s}|{:10d}\\t|{:10f}\".format(str(col),\n",
    "                                             null_values,\n",
    "                                             null_values_pct))\n",
    "\n",
    "### analyze cardinality\n",
    "def cardinality(data):\n",
    "    ''' Check number of unique values of variables\n",
    "        not accounting for null values '''\n",
    "    print(\"{:15s}\\t| {:10s}\\t| {:10s}\".format(\"Feature\",\n",
    "                                              \"Distinct Values\",\n",
    "                                              \"Distinct Values as a Proportion of Total\"))\n",
    "    print(\"=\"*100)\n",
    "    for col in data.columns[:]:\n",
    "        unique_values = len(np.unique(data[col].ffill()))\n",
    "        unique_values_pct = len(np.unique(data[col].ffill()))/len(data) \n",
    "        print(\"{:15s}\\t| {:10d}\\t\\t| {:10f}\".format(str(col),\n",
    "                                                   unique_values,\n",
    "                                                   unique_values_pct))\n",
    "\n",
    "### analyze duplicates\n",
    "def rowduplication(data):\n",
    "    ''' Assess the number and percent of duplicates for entire rows in dataset '''\n",
    "    data_dedup = data.drop_duplicates(keep='first')\n",
    "    data_duplicates = data[data.duplicated(subset=None, keep='first')]\n",
    "    \n",
    "    print('Shape of de-duplicated dataset', data_dedup.shape)\n",
    "    print('Number of duplicates:', len(data) - len(data_dedup))\n",
    "    print('Confirm number of duplicates:', len(data_duplicates))\n",
    "\n",
    "def duplicationanalyzer(data):\n",
    "    ''' Assess the number and percentage of duplicates \n",
    "        for each variable in the dataset'''\n",
    "    variable = pd.Series(np.nan)\n",
    "    \n",
    "    for var in data:\n",
    "        ''' Output the number of duplicates and proportion of duplicates '''\n",
    "        variable = data[var]\n",
    "        \n",
    "        # construct variable dataframe less duplicates\n",
    "        variable_dedup = variable.drop_duplicates(keep='first')\n",
    "        \n",
    "        #print the number of duplicates\n",
    "        duplicates=data[data[var].duplicated(keep=False)]\n",
    "        print(var)\n",
    "        print('Number of duplicates: ', len(duplicates))\n",
    "        \n",
    "        #print the percentage of duplicates\n",
    "        percentage = \"{0:.4f}\".format(len(duplicates)/len(data))\n",
    "        print('Percentage of duplicates: ', percentage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f1c0bf3-dd58-44d5-9a31-3a4556e6d185",
   "metadata": {},
   "outputs": [],
   "source": [
    "nullvalues(d=df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27c28644-8248-46b5-9789-4fc1ea34bfb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "cardinality(data=df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6defeae-7e9b-49d1-86e0-23c1c1d96bd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "rowduplication(data=df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b7cda7e-b742-4f13-9d8f-9571d53d27fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "duplicationanalyzer(data=df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf1558b9-715d-49ff-bebb-6f41ca421d14",
   "metadata": {},
   "source": [
    "#### Explore summary statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2badb382-3023-4b23-a6df-576c9ae7fe21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define functions for summary statistics on categorical and numeric variables\n",
    "\n",
    "def catvardistribution(data, var, title):\n",
    "    ''' Examine value counts and countplots'''\n",
    "    %matplotlib inline\n",
    "    fig, axs = plt.subplots(1, 1, sharey=True, tight_layout=True)\n",
    "    print(data[var].value_counts(dropna=False))\n",
    "    sns.set(style='darkgrid')\n",
    "    ax = sns.countplot(x=data[var], data=data)\n",
    "    ax.set_xticklabels(ax.get_xticklabels(), rotation=0)\n",
    "    ax.set_title(title)\n",
    "    plt.show()\n",
    "\n",
    "def summarystats(data, var, titleplot, n_bins):\n",
    "    ''' Print summary statistics, show histogram and boxplot '''\n",
    "    %matplotlib inline\n",
    "    print(data[var].unique())\n",
    "    print(data[var].describe())\n",
    "    n_bins = n_bins\n",
    "    fig, axs = plt.subplots(1, 1, sharey=True, tight_layout=True)\n",
    "    plt.hist(data[var], bins=n_bins)\n",
    "    plt.title(titleplot, loc='center', pad=None)\n",
    "    plt.show()\n",
    "#     sns.set(style='darkgrid')\n",
    "#     ax = sns.boxplot(x=data[var])\n",
    "#     print(ax)\n",
    "\n",
    "def sidebysideboxplots(data, xvar, yvar, xtitle, ytitle):\n",
    "    ''' Generate side-by-side boxplots'''\n",
    "    %matplotlib inline\n",
    "    ax = sns.boxplot(x=xvar, y=yvar, data=data)\n",
    "    ax.set_xlabel(xtitle)\n",
    "    ax.set_ylabel(ytitle)\n",
    "\n",
    "def corrmap(data,figx, figy):\n",
    "    '''Generate correlation heatmap'''\n",
    "    %matplotlib inline\n",
    "    var_corr = data.corr()\n",
    "    var_corr = var_corr.round(3)\n",
    "    # plot the heatmap and annotation on it\n",
    "    fig, ax = plt.subplots(figsize=(figx,figy))         # Sample figsize\n",
    "    sns.heatmap(var_corr, xticklabels=var_corr.columns, yticklabels=var_corr.columns, annot=True)\n",
    "\n",
    "    # Fix axes\n",
    "    b, t = plt.ylim() # discover the values for bottom and top\n",
    "    b += 0.5 # Add 0.5 to the bottom\n",
    "    t -= 0.5 # Subtract 0.5 from the top\n",
    "    plt.ylim(b, t) # update the ylim(bottom, top) values\n",
    "    plt.show()\n",
    "\n",
    "def corrmap2(data, figx, figy):\n",
    "    ''' Generate a triangular correlation heatmap '''\n",
    "    %matplotlib inline\n",
    "    corr = data.iloc[:,:].corr()\n",
    "    mask = np.triu(np.ones_like(data.iloc[:,:].corr(), dtype=bool))\n",
    "    \n",
    "    plt.figure(figsize=(figx, figy))\n",
    "    \n",
    "    # plot the heatmap\n",
    "    heatmap = sns.heatmap(corr,\n",
    "                         xticklabels=corr.columns,\n",
    "                         yticklabels=corr.columns,\n",
    "                         mask=mask,\n",
    "                         vmin=-1, vmax=1, annot=True, cmap='BrBG')\n",
    "    \n",
    "    heatmap.set_title('Correlation Heatmap', fontdict={'fontsize':8}, pad=10);\n",
    "\n",
    "def binary_means(data, feature_group, y_col):\n",
    "    ''' Show mean scores by feature category '''\n",
    "    cols = feature_dict[feature_group]\n",
    "    #cols = feat_eng\n",
    "    for col in cols:\n",
    "        print(col)\n",
    "        print(data.groupby(y_col)[col].mean())\n",
    "        print('')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d343e830-35e1-429e-bcfc-7dc3a18826fc",
   "metadata": {},
   "source": [
    "Create a list of all float, int and string variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e34dd4a-30b2-41cf-975d-12b62f9be4fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create list of float variables\n",
    "float_vars = list()        \n",
    "for x in df.columns:\n",
    "    if df[x].dtypes == 'float64':\n",
    "        float_vars.append(x)\n",
    "\n",
    "# create list of int variables\n",
    "int_vars = list()        \n",
    "for x in df.columns:\n",
    "    if df[x].dtypes == 'int':\n",
    "        int_vars.append(x)\n",
    "\n",
    "# create list of string variables\n",
    "string_vars = list()        \n",
    "for x in df.columns:\n",
    "    if df[x].dtypes == 'str':\n",
    "        string_vars.append(x)\n",
    "        \n",
    "print(float_vars)\n",
    "print(int_vars)\n",
    "print(string_vars) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc662fd0-c5f1-4e30-942b-eecb5f79bfd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save dictionary of feature lists\n",
    "feature_dict = {'strings':string_vars, 'floats':float_vars, 'ints':int_vars}\n",
    "# feature_dict = {'floats':float_vars}\n",
    "feature_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc3d0812-f6c1-42fb-b86e-20fb9c41be9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "catvardistribution(data=df, var='y', title='distribution of y')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c181adef-ce1a-4f5a-8629-582d80b95ad4",
   "metadata": {},
   "source": [
    "#### Run summary statistics on each variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33372938-763f-4539-a88c-2bd4c66312c9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for var in float_vars:\n",
    "    summarystats(data=df, var=var, titleplot=f'{var} description', n_bins=75)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13c2cf84-3149-4831-afb5-49f077c7b201",
   "metadata": {},
   "source": [
    "#### Run summary statistics on each variable when y=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bebe930f-ef32-4473-a8e7-ac713d827e8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for var in float_vars:\n",
    "    summarystats(data=df[df['y']==1], var=var, titleplot=f'{var} description when y=1', n_bins=75)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62d0576c-de53-4c12-8afc-528cc36f8c28",
   "metadata": {},
   "source": [
    "#### Run summary statistics on each variable when y=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19a2df52-ed86-4f88-8ad6-d0f8c4e14819",
   "metadata": {},
   "outputs": [],
   "source": [
    "for var in float_vars:\n",
    "    summarystats(data=df[df['y']==0], var=var, titleplot=f'{var} description when y=0', n_bins=75)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8127aff8-1f5b-4293-951c-31d2fa6936f9",
   "metadata": {},
   "source": [
    "#### Run correlation plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c2e05c5-8248-4764-ad46-fad3588ed253",
   "metadata": {},
   "outputs": [],
   "source": [
    "corrmap(data=df,figx=25, figy=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bdb8421-4883-4b05-a395-de724d6a191a",
   "metadata": {},
   "outputs": [],
   "source": [
    "corrmap2(data=df, figx=12, figy=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0a7b844-9722-4773-a95b-8ab36d9b8bbe",
   "metadata": {},
   "source": [
    "#### Examine mean value of the binary outcome variable for each feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb01e666-a9af-452e-955a-0865520523ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "binary_means(data=df, feature_group='floats', y_col='y')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3f09902-4968-4022-8072-b9b4e6f6a3be",
   "metadata": {},
   "source": [
    "### DataProfiler approach"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "762c8f62-3b95-4a4e-a18b-94f46c5c5bb2",
   "metadata": {},
   "source": [
    "Use the [dataprofiler library](https://pypi.org/project/DataProfiler/) to assess the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cff7ef21-cd80-43a6-bc14-5ef9f0b73900",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = Data(\"../data/raw_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a47fb874-ef88-492f-8f8f-9b9a45d30d84",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data.data.head(5)) # Access data directly via a compatible Pandas DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "348728ee-ba16-4736-8529-36ac4f653b01",
   "metadata": {},
   "outputs": [],
   "source": [
    "profile = Profiler(data) # Calculate Statistics, Entity Recognition, etc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e70f2f0-1d88-486f-9303-a6b0ab847fa4",
   "metadata": {},
   "source": [
    "Create a readable report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66d250cb-e118-4419-a580-b6602002fef3",
   "metadata": {},
   "outputs": [],
   "source": [
    "readable_report = profile.report(report_options={\"output_format\": \"compact\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "011931ca-598b-4cd2-a9a9-03e8aa5edac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "readable_report"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5baaa63d-4103-47d3-a208-528f8e725238",
   "metadata": {},
   "source": [
    "## Use Pandas Profiler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35433096-0ec5-47a8-a184-998c34f523b1",
   "metadata": {},
   "source": [
    "Pandas Profiling is a Python package that can be used to automate EDA. It is an excellent tool for producing reports in an interactive HTML style that makes it simple to read and analyse the data. Let's investigate Pandas Profiling to perform EDA quickly and with just one line of code.With only a few lines of code, you can generate dynamic, interactive collections of exploratory data analysis (EDA) tables and visualisations using the Pandas Profiling Library. \n",
    "\n",
    "See Github of Python package [here](https://github.com/ydataai/ydata-profiling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66023f68-d1c8-4b37-bace-82b4c41dc444",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ydata_profiling import ProfileReport\n",
    "\n",
    "# generate standard profiling report\n",
    "profile = ProfileReport(df, title=\"Profiling Report\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c30b8ced-40c0-44f9-a7af-dad9f5f6979d",
   "metadata": {},
   "source": [
    "## Use Data Prep"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67a08942-427f-41f1-a356-1c7c32d4bf06",
   "metadata": {},
   "source": [
    "The DataPrep library lets you prepare your data using a single library with a few lines of code. One can use DataPrep to: - Collect data from common data sources (through Connector) - Do your exploratory data analysis (through EDA) - Clean and standardize data (through Clean)\"DataPrep.Connector\" provides an intuitive, open-source API wrapper that speeds up development by standardizing calls to multiple APIs as a simple workflow. Streamline calls to multiple APIs through one intuitive library. It also supports loading data from databases through SQL queries. With one line of code, you can speed up pandas.read_sql by 10X with 3X less memory usage!\"DataPrep.EDA\" is the fastest and the easiest EDA tool in Python. It allows data scientists to understand a Pandas/Dask DataFrame with a few lines of code in seconds.\"DataPrep.Clean\" aims to provide a large number of functions with a unified interface for cleaning and standardizing data of various semantic types in a Pandas or Dask DataFrame.Here we will keep the scop limited to DataPrep.EDA. See the library documentation [here](https://pypi.org/project/dataprep/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdb5bb61-3cd6-40c9-841d-4767cf248b06",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataprep.eda import create_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8c8eb66-a520-47ee-99d8-7dcc0ef61ea5",
   "metadata": {},
   "outputs": [],
   "source": [
    "create_report(df).show_browser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acc20b10-a1ce-4923-97f4-c8f6fdabf58a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
